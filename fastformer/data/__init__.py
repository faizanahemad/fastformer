from .dataset import TokenizerDataset, collate_fn, char_to_id
from .sample_data import very_large_texts, large_texts
